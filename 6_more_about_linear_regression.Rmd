---
title: "Untitled"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Подробнее о линейной регрессии

Есть куча методов

![Классификация методов статистического вывода](img/table.png)

Но оказывается всё можно свести к тому или иному выводу регрессионого анализа.

![Классификация методов статистического вывода](img/table.png)

Регрессия это очень важный пункт. От него потом легче понимать машинное обучение.

## Ограничения 

* Линейность взаимосвязи

Рассмотрим пример:

```{r}
library(tidyverse)
data(mtcars)
ggplot(mtcars, aes(hp, mpg)) + geom_point()
```

Мы можем записать уравнение линейной регрессии $mpg = b_0 + b_1 hp$

```{r}
ggplot(mtcars, aes(hp, mpg)) + geom_point() + geom_smooth(method = "lm")
```

Но и на графике, и по ошибкам на прямой выше, видно, что связь скорее всего не линейная

Поэтому в подобных случаях линейная регрессия не лучшее решение. Особенно, если мы хотим предсказывать значения.

Как бороться? Лучше сразу строить данные. Есть способы --- можно модифицировать зависимость.

### Трансформация Тьюки

Основная идея --- трансформировать независимую переменную (предиктор), чтобы  ликвидировать нелинейность связи. Возведением в степень.

Нюанс, если показатель степени $\lambda$ ниже нуля, то будем подставлять минус, чтобы перевернуть

![Классификация методов статистического вывода](img/tyki.png)


