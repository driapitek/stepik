---
title: "Untitled"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Подробнее о линейной регрессии

Есть куча методов

![Классификация методов статистического вывода](img/table.png)

Но оказывается всё можно свести к тому или иному выводу регрессионого анализа.

Регрессия это очень важный пункт. От него потом легче понимать машинное обучение.

### Ограничения 

![_](img/ogran.PNG)

* Линейность взаимосвязи ЗП и НП

Рассмотрим пример:

```{r}
library(tidyverse)
data(mtcars)
ggplot(mtcars, aes(hp, mpg)) + geom_point()
```

Мы можем записать уравнение линейной регрессии $mpg = b_0 + b_1 hp$

```{r}
ggplot(mtcars, aes(hp, mpg)) + geom_point() + geom_smooth(method = "lm")
```

Но и на графике, и по ошибкам на прямой выше, видно, что связь скорее всего не линейная

Поэтому в подобных случаях линейная регрессия не лучшее решение. Особенно, если мы хотим предсказывать значения.

Как бороться? Лучше сразу строить данные. Есть способы --- можно модифицировать зависимость.

### Трансформация Тьюки

Основная идея --- трансформировать независимую переменную (предиктор), чтобы  ликвидировать нелинейность связи. Возведением в степень.

Нюанс, если показатель степени $\lambda$ ниже нуля, то будем подставлять минус, чтобы перевернуть

![Классификация методов статистического вывода](img/tyki.png)

Давайте реализуем несколько преобразований

```{r}
ggplot(mtcars, aes(hp^0.5, mpg)) + geom_point()
ggplot(mtcars, aes(hp^-0.5, mpg)) + geom_point()
ggplot(mtcars, aes(-hp^-0.5, mpg)) + geom_point()
```

Модели с трансформацией

```{r}
fit1 <- lm(mpg ~ hp, mtcars)
fit2 <- lm(mpg ~ I(-hp^-0.7), mtcars)
```


### Логарифмическая трансформация переменных

Что будет, если мы применим логарифмирование одновременно и к зависимой переменной и к независимой

```{r}
qplot(data = mtcars, x = log(hp), y = log(mpg))
```

На самом деле мы вроде как справились с нелинейностью взаимосвязи.

Создадим 

```{r}
fit3 <- lm(log(mpg)~log(hp), mtcars)
summary(fit3)
```

Что говорит коэффициент при log(hp) --- у нас при единичном изменении log(hp) значение log(mpg) будет уменьшаться на приблизительно 0.5. 
Можно перейти от оценки логарифмов к обычным значениям.

Вспмомним мат.основы регрессии. Если порядок иксов будет изменяться на единицу, то изменение предсказаний будет равно коэффициенту при иксах:

![_](img/coef_pri_iks.PNG)

Что теперь? у нас исходная модель изменилась. У нас теперь логарифмы.

Теперь у нас коэффициент бэ один показывает на сколько процентов увеличится исходной переменной предиктор изменится на один процент

![_](img/dokaz.PNG)

Т.е. как пример разберём вот такую ситуацию:

![_](img/primer.PNG)

#### Вывод

Когда используем трансформацию и зависмой и независимой переменной, мы говорим, что коэффициент при логарифме икс это то на сколько процентов увеличится зависимой переменной, при условии что значение независимой переменной увеличится на один процент.

Т.е. в случае модели fit3 --- при единичном увеличении лошадинных сил на один  процент  значение расхода топлива уменьшита на пол.процента (-0.53009)

Т.е. если посмотреть на график рассеяния нелогарифмированных величин, то там видно что при нам такая трактовка и говорит что связь не линейна. Потому что чем дальше от исходного значения тем шире нужно сделать шажок для того чтобы было справедливо высказывание об угасании на пол.процента

Говоря об остальных трансформациях предикторов или зависимой переменной

![_](img/interpretation.PNG)


### Распределение остатков

Очень часто, когда нелинейное распределение, у нас получается не нормальное распределение остатков.

Посмотрим на распределение остатков в наших моделях

```{r}
hist(fit1$residuals)
hist(fit2$residuals)
hist(fit3$residuals)
```

так сразу и не скажешь, но мы можем проверить при помощи тестов на нормальность. Наппример тест Шапиро-Вилко

```{r}
shapiro.test(fit1$residuals)
shapiro.test(fit2$residuals)
shapiro.test(fit3$residuals)
```

Видно что p уровень значимости всё возрастает, и если во второй и третьей моделях у нас нет оснований отвергуть нулевую гипотезу.
У нас в последней модели всё ещё лучше 


Когда есть подозрение о том что предикторы взаимосвязаны нелинейно с независимой переменной. Очень часто происходит так, что если мы посмотрим на распределение остатков, то оно будет не нормальным.

Самое правильное решение при этом --- трансформация переменных.

Способы описаны выше. Есть один из методов

![_](img/box-cox.PNG)

### Промежуточный итог

Мы разобрали две проблемы --- нелинейность зависимости ЗП и НП. Ненормальность остатков 

Степенная трансформация переменных в регрессии не всегда позволит достичь увеличения качества регрессионной мождели (значения R квадрат).

Для некоторых значений показателя степени необходимо, чтобы значения переменной были неотрицательны.

Трансформация при помощи логарифма далеко не всегда позволит сделать взаимосвязь более линейной.

При использовании показателя степени меньше нуля мы берем значения переменной с другим знаком, чтобы сохранить исходное направление взаимосвязи


