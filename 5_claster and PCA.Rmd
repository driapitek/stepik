---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Кластерный анализ и метод главных компонент

Два метода сами находят правильный ответ. Как всё это происходит?

## Постановка задачи

Представим исследование, в котором провели опрос людей и фиксировали некоторые параметры:

* Испытуемый

* Возраст

* Доход

* Стаж работы

* Знания R 

* Знания статистики

При некоторых группировках данных в визуализации появляются разные группы.

*Что будет делать кластерный анализ:*

Кластерный анализ будет бежать по нашим наблюдениям и пытаться ответить на вопрос --- есть ли у нас данных группировка наблюдений по разным кластерым. И сколько таких кластеров разумнее всего выделить.

*Что будет метод главных компонент:*

PCA будет смотреть на переменные и говорить, можно ли сократить размерность наших данных. Мы научимся группировать некоторые переменные. Мы научимся описывать взаимосвязанные переменные в интегративные переменные. Мы будем сокращать размерность.

![Постановка задачи](img/claster_PCA.PNG)

## Кластерный анализ методом k-средних

При помощи кластерного анализа мы попробуем выяснить есть ли подгруппы в данных.

Это обучение без учителя --- мы изначально не знаем, есть ли какие-то подгруппы.

Регрессионный анализ --- обучение с учителям.

Перейдём к данным Ирис

```{r}
library(tidyverse)

data(iris)
```

Изобразим зависимость длины лепестка от толщины чашелистника

```{r}
ggplot(iris, aes(Sepal.Length, Petal.Width)) + geom_point()
```

Если мы посмотрим, то чисто на глаз, можно сказать что разделить можно на три вида --- снизу три, посередине и справа сверху.

Как мы можем решить задачу? Как раз методом k-средних

### Идея метода

* Решаем на сколько кластеров будем делить наблюдения

* Случайно выбираем начальные позиции цетроидов кластера

* Выводим центроиды на наилучшие позиции

Метод сам по себе не знает сколько кластеров.

Что такое центроида кластеров? Чтобы нести поднос со стаканами на одном пальце, нужно поставить палец в центр тяжести.

Как рассчитать центроиду? Чтобы найтри центроиду кластера можно рассчитать среднее арифметическое по двум измерениям в точках кластера.

### Как работает метод k-means

Метод k-means на первом этапе получает количество кластеров, на которое нужно поделить дата сет:

![Разбор метода k-means](img/kmeans.PNG)

Далее мемтод берёт 5 случайных точек и говорит что это центроиды кластеров.

![Разбор метода k-means](img/kmeans1.PNG)

Теперь для каждого наблюдения определяем к какому центроиду он ближе всего. Для этого будем использовать геометрическое расстояние до точки. После этого скорректируем центроиды:

![Разбор метода k-means](img/kmeans2.PNG)

Но теперь, как только мы подвинули центриды, мы изменили баланс сил --- те точки, которые были раньше ближе к одним точкам, теперь ближе к другим. 

Т.е. теперь мы можем переопределить принадлежность класса. Итого:

1. Случайно выбрали цетроиды для указанного класса

2. Поместили в геометрический центр точек класса

3. Переопределили точки

4. Обновим положение центроид

5. когда остановится? Делаем до тех пор пока после очередного обновления центроидов ни одна точка не поменяет свою принадлежность.


## K-means продолжение

[визуализация работы алгоритма](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)

Вопрос номер один --- как понять что наш алгоритм сошёлся.

2. Правильно ли он сошёлся?

3. Какое число кластеров оптимальнее всего выбрать.

### Особенность
Действительно, интересной особенностью кластерного анализа k - means является тот факт, что он включает в себя элемент случайности при выборе исходных позиций центроидов. В результате при многократном повторении кластеризации на одних и тех же данных мы можем получать различные варианты кластерного решения. Чем менее явно представлена в наших данных кластерзация наблюдений, тем более существенными могут оказаться различия. 

```{r}
library(ggplot2)
d <- iris[, c("Sepal.Length", "Petal.Width")]

fit <- kmeans(d, 3)
d$clusters <- factor(fit$cluster)

ggplot(d, aes(Sepal.Length, Petal.Width, col = clusters))+
    geom_point(size = 2)+
    theme_bw() 
```


### Может ли кластерный анализ "ошибаться"?

Может. Вот например иллюстрация:

![Расчет ожидаемых значений](img/wrong_claster.png)

Что делать?

1. Первый способ --- воздействовать на первоначальное размещение центроид. Например, первоначальные точки ставить максимально далеко друг от друга. Но даже в этом случае можно не сойтись.

2. Второй способ --- многократное повторение алгоритма.

### Как выбрать число кластеров?

Можно выбрать оптимальное число кластеров.

В качестве понятия оптимальности будем использовать два понятия:

* Внутригрупповая сумма квадратов --- сумма квадратов отклонений каждого наблюдения от центроида кластера

* Общая внутригрупповая сумма квадратов --- будет получена если мы для каждого класстера рассчитаем его внутригрупповую сумму квадратов а потом просуммируем эти показатели.

Посдений показатель мы и будем использовать для оценки качества кластеризации.

Как это работает?

Идея --- если добавление ещё одного кластера в наши данные значительно понижает общую сумму квадртов --- это означает, что в увеличении числа кластеров есть смысл.

Если в структуре данных нет взаимосвязи, то уменьшение общей внутригруппово суммы квадратов будет происходить плавно

![Расчет ожидаемых значений](img/within_sum_claster.png)

Посмотрим, что происходит, если в данных есть структура.

![Расчет ожидаемых значений](img/good_claster.png)


Если общая сумма плавно убывает --- значит нет структуры данных.

Если общая сумма скачкообразно изменяется --- значит количество кластеро выбрано оптимально.

В общей ситуации нужно искать оптимальное количество кластеров в точке излома графика общей суммы квадратов внутри групп.

Это общий подход.

Можно для определения оптимального числа кластеров воспользоваться пакетом NbClust:


```{r}
# install.packages("NbClust")
library(NbClust)
data(iris)
dt <- iris[, 1:4]
N <- NbClust(dt, distance = "euclidean",
             min.nc = 2, max.nc = 15, method = "complete", 
             index = "alllong")
```


### Задача 

Предположим есть один следующий кластер. Чему равна внутрикластерная сумма квадратов

```{r}
df <- data.frame(x=c(-3,1,2,3,5,6,7), y = c(3,4,6,8,2,11,1))
ggplot(df, aes(x,y)) + geom_point()
```

Всё просто:

```{r}
RES <- kmeans(df,1)
RES$withinss
```


## Иерархическая кластеризация

Метод ближайшего соседа (одиночной связи)

С самого начала алгоритм считает, что число кластеров = числу наблюдений.

Мы находим точки, которые ближе всего к друг другу и заменяет их центроидой.

Затем мы повторяем подход и считаем расстояние не до первых двух а уже для центроиды этих точек

![Метод ближайшего соседа](img/single.png)

И так повторяем  до тех пор пока не объединим все точке в кластер. Так и работает этот метода

Есть метод дальнего соседа --- он в последнюю очередь объединяет самые дальние в последнюю очередь.

На момент начала иерархической кластеризации ОВСК равна нулю (кластеров ровно столько, сколько наблюдений, и центроид каждого кластера совпадает с соответствующим наблюдением). А объединять надо такие кластеры, слияние которых приведет к наименьшему приросту ОВСК.

Полезно рисовать дендрограммы. Например вот тут нет чётко выраженной структуры:

![Метод ближайшего соседа](img/dendrogram.png)

### Итог

Можно комбинировать указанные способы кластеризации.

Особое внимание стоит уделить тому, как происходит выбор числа кластеров при иерархической кластеризации. Как я сказал, в этом случае у нас есть некоторая свобода в том, какое число кластеров выбрать, но то как исходные данные разделятся на выбранное число кластеров все-таки подчиняется логике работы алгоритма. Рассмотрим пример кластеризации следующего набора данных:

![Метод ближайшего соседа](img/dendrogram1.png)

Например, если мы проведем данную кластеризацию в R и попросим выделить нам два кластера, то получим следующее решение:

![Метод ближайшего соседа](img/dendrogram2.png)

Мы могли бы продолжить наше движение вниз, срезая все больше веток, и разделяя наши данные все на большее число кластеров.

```{r}
library(ggplot2) 
library(ggrepel) # для симпатичной подписи точек на графике

x <- rnorm(10)
y <- rnorm(10)
test_data <- data.frame(x, y)
test_data$labels <- 1:10

ggplot(test_data, aes(x, y, label = labels))+
    geom_point()+
    geom_text_repel()

d = dist(test_data)
fit <- hclust(d, method = "single")
plot(fit, labels = test_data$labels)
rect.hclust(fit, 2) # укажите желаемое число кластеров, сейчас стоит 2
```

## Введение в метод главных компонент

Предположим мы набрали 40 человек, и узнали как коррелирует знания статистики  со знанием R.

![Метод главных компонент](img/PCA.png)

Что значит найти главную компоненту  наших данных? Главных компонент всегда столько же, сколько исходных переменных. Основная идея в том, чтобы нарисовать новые оси, которые лучше отображают изменчивость наших данных.

Итак, сейчас каждая точка задаётся двумя параметрами --- проекциями на оси абсцисс и ординат.

![Метод главных компонент](img/PCA1.png)

Теперь зададимся вопросом --- можем ли мы нарисовать на графике такую линию, проекция на которую макисмально точно предсказывать в каком месте будет наша точка на графике.

Это будет наша регрессионная прямая.

Регрессионная прямая будет служить нам новой осью. И теперь мы будем смотреть расстояние не до первоначальных осей а до новой оси.

![Метод главных компонент](img/PCA2.png)

Таким образом мы решаем обратную задачу: раньше мы зная корреляцию могли по значению одной переменной предсказать значение другой переменной.

А теперь мы проведя прямую восстанавливаем исходные значения изначальных переменных у этой точки. Если мы знаем что на новой компоненте высокой значение точки. То мы знаем, что и на первых двух осях значения будет большие. 

Посмотрим сколько бы измен чивости объяснила главная компонента.

![Метод главных компонент](img/PCA2.png)
Теперь осталось построить вторую ось --- она перпендикулярна второй оси

![Метод главных компонент](img/PCA3.png)

ТЕперь все наши данные объясняются на 100%.

![Метод главных компонент](img/PCA4.png)


### График biplot

Этот график показывает и исходные переменные и две новые и пространство точек: 

![Метод главных компонент](img/biplot.png)

Как его интерпетировать?

1. Мы просто перешли в новые оси --- две новые компоненты теперь на местах икса и игрека. Первая объясняет большую часть изенчивости данных, вторая под прямым углом, чтобы забрать всю оставшуюся часть необъяснённых данных.

2. Что касается самих переменных --- старые перменные это вектора. Угол между ними определяет их корреляцию. Если угол острый --- корреляция положительная. Если кгол тупой --- корреляция отрицательная.

3. Исходные оси сонаправлены с PC1 первой главной компонентой. Это говорит о том, что положительное изменение исходных данных одинаково влияет на изменение всех этих трёх параметров.

Давайте рассмотрим точку 7.

Что про неё можно сказать? что у неё самое большое значение по главной компоненте 1. Это говорит что  наш 7 человек имеет большие показатели и по R и по Stat

![Метод главных компонент](img/biplot1.png)

Тоже справедливо для обратных точек (19, 29).

### Снижение размерности

Что мы можем сделать? Вот есть исходные данные. Мы в наши данные можем добавить новую компоненту. Назовём её допустим "опыт в анализе данных" и теперь можно использовать её для объяснения данных

![Метод главных компонент](img/biplot2.png)

### Зачем всё это нужно?

Такая задача возникает часто, особенно когда перменных много.

Рассмомотрим пример, когда перменных больше двух

```{r}
library(tidyverse)
data(swiss)
```

```{r}
fit <- prcomp(swiss, center = T)
plot(fit, type = "l")
summary(fit)
biplot(fit)
```

Две первых компоненты в сумме объясняют нам 92% всех исходных данных.

![Метод главных компонент](img/biplot3.png)

И более того, в пространстве этих двух компонент, мы можем сказать как связаны между собой исходные параметры.

Теперь мы можем скомбинировать этот метод с кластерным анализом.

Во вторых можно искать отличающиеся точки и чем они отличаются.

В данном примере мы 6-мерное пространство отобразили на двух-мерном. 

Можно конечно быть более скрупулёзным и построить 3ёх мерное пространство

```{r}
# install.packages("pca3d")
library(pca3d)

dt <- swiss
dt$is_catholic <- ifelse(swiss$Catholic > 50, 1, 0)
dt$is_catholic <- factor(dt$is_catholic)
fit <- prcomp(swiss, center = T)
pca3d(fit, group = dt$is_catholic,
      fancy = T, 
      new=T)
```


### Задача

Напишите функцию smart_hclust, которая получает на вход dataframe  с произвольным числом количественных переменных и число кластеров, которое необходимо выделить при помощи иерархической кластеризации.

Функция должна в исходный набор данных добавлять новую переменную фактор - cluster  -- номер кластера, к которому отнесено каждое из наблюдений.

#### Решение

Подготовим тестовые данные

```{r}
test_data <- read.csv("https://stepic.org/media/attachments/course/524/test_data_hclust.csv")
```

Для расчета матрицы расстояний предполагается, что используется функция dist() также с параметрами по умолчанию:

```{r}
dist(test_data, method = "euclidean", diag = FALSE, upper = FALSE, p = 2)
```


Итак для того чтобы рассчитать кластеры нам нужно.

1. Построить матрицу расстояний

```{r}
dist_matrix <- dist(test_data, method = "euclidean", diag = FALSE, upper = FALSE, p = 2)
```

2. Провести иерархическую кластеризацию

```{r}
fit <- hclust(dist_matrix)
```

3. Теперь можно получить номер кластера для каждого наблюдения

```{r}
cluster <- cutree(fit, 3)
```

Итого скомпануем в функцию

```{r}
smart_hclust <-  function(test_data, cluster_number){
dist_matrix <- dist(test_data, method = "euclidean", diag = FALSE, upper = FALSE, p = 2)
fit <- hclust(dist_matrix)
cluster <- cutree(fit, cluster_number)
test_data$cluster <- as.factor(cluster)
return(test_data)
}

smart_hclust(test_data, 5)
```

### Задача

Интересной особенностью кластерного анализа является тот факт, что мы получаем только итоговый ответ, к какому кластеру принадлежит каждое наблюдение. Однако мы не знаем, по каким переменным различаются выделенные кластеры. Поэтому, если нас интересует не только сам факт того, что мы смогли выделить кластеры в наших данных, но мы также хотим понять, чем же они различаются, разумно сравнить кластеры между собой по имеющимся переменным.

Напишите функцию get_difference, которая получает на вход два аргумента: 

* test_data — набор данных с произвольным числом количественных переменных.

* n_cluster — число кластеров, которое нужно выделить в данных при помощи иерархической кластеризации.

Функция должна вернуть названия переменных, по которым были обнаружен значимые различия между выделенными кластерами (p < 0.05). Иными словами, после того, как мы выделили заданное число кластеров, мы добавляем в исходные данные новую группирующую переменную — номер кластера, и сравниваем получившиеся группы между собой по количественным переменным при помощи дисперсионного анализа.

Пример работы функции:

В первом наборе данных, очевидно, что два кластера будут значимо различаться только по переменной V2.


#### Решение

Сама функция

```{r}
get_difference <-  function(test_data, n_cluster){
  #1
  dist_matrix <- dist(test_data, method = "euclidean", diag = FALSE, upper = FALSE, p = 2)
  #2
  fit <- hclust(dist_matrix)
  #3
  test_data$cluster <- as.factor(cutree(fit, n_cluster))
  #4
  only_numeric <- test_data[, sapply(test_data, is.numeric)]
  #5
  p_value <- sapply(only_numeric, function(x)(summary(aov(x~cluster, test_data))[[1]]$'Pr(>F)'))[1,]
  #6
  index <- p_value < 0.05
  #7
  factor <- colnames(test_data)[-length(test_data)][index]
  return(factor)
}
```


Разберём её работу.

Для решения используем шаги предыдущей функции.

1. РАссчитываем матрицу расстояний

2. Проводим иерархическую кластеризацию

3. Показываем к какому кластеру относится измерение

4. Для дисперсионного анализа отбираем только численные, исходные переменные

5. Применяем к численным переменным дисперсионный анализ, и создаём вектор из рассчитанных значений p_value

6. Проверяем можно ли отвергнуть нулевую гипотезу

7. Выводим имена переменных, по которым были обнаружен значимые различия между выделенными кластерами

Тестируем

```{r}
test_data_cluster <- read.csv("https://stepic.org/media/attachments/course/524/cluster_2.csv")
get_difference(test_data_cluster, 2)
```

Вот более короткое и красивое решение

```{r}
get_difference <- function(df, n_clusters){
    fit <- hclust(dist(df))
    cluster <- factor(cutree(fit, n_clusters))
    is.good <- sapply(df, function(x) anova(aov(x ~ cluster))$P[1] < 0.05)
    names(df)[is.good]
}
```

### Задача

Напишите функцию get_pc, которая получает на вход dataframe с произвольным числом количественных переменных. Функция должна выполнять анализ главных компонент и добавлять в исходные данные две новые колонки со значениями первой и второй главной компоненты. Новые переменные должны называться "PC1"  и "PC2" соответственно.

#### Решение

Подготовим тестовые данные

```{r}
test_data <- read.csv("https://stepic.org/media/attachments/course/524/pca_test.csv")
```


```{r}
fit <- prcomp(test_data, center = T)

str(fit)

test_data$PC1 <- fit$x[,1]
test_data$PC2 <- fit$x[,2]

get_pc <- function(d){
fit <- prcomp(d, center = T)
d$PC1 <- fit$x[,1]
d$PC2 <- fit$x[,2]
return(d)
}

get_pc(test_data)

```

Вот ещё вариант более общего решения

```{r}
get_pc <- function(test){    
	fit <- prcomp(test)    
	test<- cbind(test, fit$x[,1:2])    
	return(test)    
}
```

### Задача

Усложним предыдущую задачу! Напишите функцию get_pca2, которая принимает на вход dataframe с произвольным числом количественных переменных. Функция должна рассчитать, какое минимальное число главных компонент объясняет больше 90% изменчивости в исходных данных и добавлять значения этих компонент в исходный dataframe в виде новых переменных.


#### Решение

Рассмотрим работу функции на примере встроенных данных `swiss`:

Посмотрим сколько главных компонент объясняют больше 90% изменчивости в данных

```{r}
fit <- prcomp(swiss)
```

в нашем случае уже две компоненты объясняют больше 90% изменчивости. значит значения первых двух компонент нужно добавить в исходные данные.


```{r}
get_pca2 <- function(data){
  fit <- prcomp(data)
  vars <- apply(fit$x, 2, var)
  props <- vars / sum(vars)
  index <- sum(cumsum(props) < 0.9) + 1
  data<- cbind(data, fit$x[,1:index])  
  return(data)
}

result  <- get_pca2(swiss)
str(result)
```


Как работает функция

1. Берём модель

2. Считаем вручную дисперсию (функция apply). Она берёт из модели пересчитанные компоненты и считает при помощи `var` дисперсию

3. Считаем кумулятивную сумму

4. берём индекс + 1 который удовлетворяет условию

5. Вносим в исходную выборку компоненты, удовлетворяющие условию


### Задача

Задача для Чака Норриса.

Как я говорил, метод главных компонент может применяться для борьбы с мультиколлинеарностью в данных (ситуация, когда некоторые переменные очень сильно коррелируют между собой). Однако иногда некоторые переменные не просто сильно взаимосвязаны, но могут представлять линейную комбинацию друг друга. На такие переменные лучше сразу взглянуть повнимательнее и выяснить, откуда они взялись в наших данных.

Напишите функцию is_multicol, которая получает на вход dataframe произвольного размера с количественными переменными. Функция должна проверять существование строгой мультиколлинеарности, а именно наличие линейной комбинации между предикторами. Линейной комбинацией является ситуация, когда одна переменная может быть выражена через другую переменную при помощи уравнения $V_1=k∗V_2+b$.
Например V1 = V2 + 4 или V1 = V2 - 5.

Функция возвращает имена переменных, между которыми есть линейная зависимость или cобщение "There is no collinearity in the data".

#### Решение

```{r}
test_data <- read.csv("https://stepic.org/media/attachments/course/524/Norris_2.csv")
```


```{r}
zero_range <- function(x, tol = .Machine$double.eps ^ 0.5) {
  if (length(x) == 1) return(TRUE)
  x <- range(x) / mean(x)
  isTRUE(all.equal(x[1], x[2], tolerance = tol))
}
```



```{r}

v <- test_data[,1] - test_data[,1]

data.frame("V1-V1",t(v))

test_data[,1] - test_data[,1:4]


test_data[,1] - test_data[,2]
test_data[,1] - test_data[,3]
test_data[,1] - test_data[,4]

test_data[,2] - test_data[,1]
test_data[,2] - test_data[,2]
test_data[,2] - test_data[,3]
test_data[,2] - test_data[,4]

test_data[,3] - test_data[,1]
test_data[,3] - test_data[,2]
test_data[,3] - test_data[,4]

test_data[,4] - test_data[,1:4]

```

Пробуем циклом фор

```{r}

list <- list()

for(i in seq_along(test_data)){
  list[[i]] <- test_data[, i] - test_data[, 1:length(test_data)]
}

list[[2]]


length(unlist(list))

unique(names(unlist(list)))

score <- matrix(abs(unlist(list)), nrow = nrow(test_data))

dimnames(score) <- list(1:3, col_names)

```



```{r}
for(i in seq_along(test_data)){
  m <- test_data[, i] - test_data[, 1:i]
}

for(i in seq_along(test_data)){
  m[[i]] <- test_data[, i] - test_data[, 1:length(test_data)]
}

```

Подход к решению такой

1. Нахожу поочерёдно разницу всеми столбцами
